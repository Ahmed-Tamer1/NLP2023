{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\nimport re\n\nprint(\"Tensorflow Version\",tf.__version__)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2023-05-06T16:18:04.494052Z","iopub.execute_input":"2023-05-06T16:18:04.494436Z","iopub.status.idle":"2023-05-06T16:18:05.522466Z","shell.execute_reply.started":"2023-05-06T16:18:04.494371Z","shell.execute_reply":"2023-05-06T16:18:05.521326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv',\n                 encoding = 'latin',header=None)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:05.524691Z","iopub.execute_input":"2023-05-06T16:18:05.52532Z","iopub.status.idle":"2023-05-06T16:18:11.761981Z","shell.execute_reply.started":"2023-05-06T16:18:05.525015Z","shell.execute_reply":"2023-05-06T16:18:11.760889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:11.763258Z","iopub.execute_input":"2023-05-06T16:18:11.763603Z","iopub.status.idle":"2023-05-06T16:18:11.786787Z","shell.execute_reply.started":"2023-05-06T16:18:11.763551Z","shell.execute_reply":"2023-05-06T16:18:11.785274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(['id', 'date', 'query', 'user_id'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:11.789615Z","iopub.execute_input":"2023-05-06T16:18:11.791766Z","iopub.status.idle":"2023-05-06T16:18:11.830873Z","shell.execute_reply.started":"2023-05-06T16:18:11.791716Z","shell.execute_reply":"2023-05-06T16:18:11.830102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lab_to_sentiment = {0:\"Negative\", 4:\"Positive\"}\ndef label_decoder(label):\n  return lab_to_sentiment[label]\ndf.sentiment = df.sentiment.apply(lambda x: label_decoder(x))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:11.834279Z","iopub.execute_input":"2023-05-06T16:18:11.834779Z","iopub.status.idle":"2023-05-06T16:18:12.296848Z","shell.execute_reply.started":"2023-05-06T16:18:11.834615Z","shell.execute_reply":"2023-05-06T16:18:12.296123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_count = df.sentiment.value_counts()\n\nplt.figure(figsize=(8,4))\nplt.bar(val_count.index, val_count.values)\nplt.title(\"Sentiment Data Distribution\")","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:12.299816Z","iopub.execute_input":"2023-05-06T16:18:12.300213Z","iopub.status.idle":"2023-05-06T16:18:12.525122Z","shell.execute_reply.started":"2023-05-06T16:18:12.300153Z","shell.execute_reply":"2023-05-06T16:18:12.524019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nrandom_idx_list = [random.randint(1,len(df.text)) for i in range(10)] # creates random indexes to choose from dataframe\ndf.loc[random_idx_list,:].head(10) # Returns the rows with the index and display it","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:12.526741Z","iopub.execute_input":"2023-05-06T16:18:12.527306Z","iopub.status.idle":"2023-05-06T16:18:12.66589Z","shell.execute_reply.started":"2023-05-06T16:18:12.527108Z","shell.execute_reply":"2023-05-06T16:18:12.665171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\n\ntext_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:12.667837Z","iopub.execute_input":"2023-05-06T16:18:12.668148Z","iopub.status.idle":"2023-05-06T16:18:12.676768Z","shell.execute_reply.started":"2023-05-06T16:18:12.668102Z","shell.execute_reply":"2023-05-06T16:18:12.675921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(text, stem=False):\n  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n  tokens = []\n  for token in text.split():\n    if token not in stop_words:\n      if stem:\n        tokens.append(stemmer.stem(token))\n      else:\n        tokens.append(token)\n  return \" \".join(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:12.680165Z","iopub.execute_input":"2023-05-06T16:18:12.680489Z","iopub.status.idle":"2023-05-06T16:18:12.688068Z","shell.execute_reply.started":"2023-05-06T16:18:12.680442Z","shell.execute_reply":"2023-05-06T16:18:12.686915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.text = df.text.apply(lambda x: preprocess(x))","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:18:12.689527Z","iopub.execute_input":"2023-05-06T16:18:12.689899Z","iopub.status.idle":"2023-05-06T16:19:05.189371Z","shell.execute_reply.started":"2023-05-06T16:18:12.689831Z","shell.execute_reply":"2023-05-06T16:19:05.18836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_SIZE = 0.8\nMAX_NB_WORDS = 100000\nMAX_SEQUENCE_LENGTH = 30","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:05.190663Z","iopub.execute_input":"2023-05-06T16:19:05.191142Z","iopub.status.idle":"2023-05-06T16:19:05.203186Z","shell.execute_reply.started":"2023-05-06T16:19:05.191092Z","shell.execute_reply":"2023-05-06T16:19:05.201126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE,\n                                         random_state=7) # Splits Dataset into Training and Testing set\nprint(\"Train Data size:\", len(train_data))\nprint(\"Test Data size\", len(test_data))","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:05.206401Z","iopub.execute_input":"2023-05-06T16:19:05.206909Z","iopub.status.idle":"2023-05-06T16:19:05.498179Z","shell.execute_reply.started":"2023-05-06T16:19:05.206864Z","shell.execute_reply":"2023-05-06T16:19:05.49709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:05.499448Z","iopub.execute_input":"2023-05-06T16:19:05.500034Z","iopub.status.idle":"2023-05-06T16:19:05.512041Z","shell.execute_reply.started":"2023-05-06T16:19:05.499975Z","shell.execute_reply":"2023-05-06T16:19:05.510779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data.text)\n\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary Size :\", vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:05.513596Z","iopub.execute_input":"2023-05-06T16:19:05.514441Z","iopub.status.idle":"2023-05-06T16:19:26.245292Z","shell.execute_reply.started":"2023-05-06T16:19:05.514298Z","shell.execute_reply":"2023-05-06T16:19:26.244353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nx_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text),\n                        maxlen = MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text),\n                       maxlen = MAX_SEQUENCE_LENGTH)\n\nprint(\"Training X Shape:\",x_train.shape)\nprint(\"Testing X Shape:\",x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:26.246623Z","iopub.execute_input":"2023-05-06T16:19:26.247171Z","iopub.status.idle":"2023-05-06T16:19:56.742513Z","shell.execute_reply.started":"2023-05-06T16:19:26.247117Z","shell.execute_reply":"2023-05-06T16:19:56.741673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train_data.sentiment.unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:56.743805Z","iopub.execute_input":"2023-05-06T16:19:56.744169Z","iopub.status.idle":"2023-05-06T16:19:56.800349Z","shell.execute_reply.started":"2023-05-06T16:19:56.744118Z","shell.execute_reply":"2023-05-06T16:19:56.799625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(train_data.sentiment.to_list())\n\ny_train = encoder.transform(train_data.sentiment.to_list())\ny_test = encoder.transform(test_data.sentiment.to_list())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:56.801674Z","iopub.execute_input":"2023-05-06T16:19:56.802043Z","iopub.status.idle":"2023-05-06T16:19:57.528108Z","shell.execute_reply.started":"2023-05-06T16:19:56.801994Z","shell.execute_reply":"2023-05-06T16:19:57.527081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!wget http://nlp.stanford.edu/data/glove.6B.zip\n#!unzip glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:57.529562Z","iopub.execute_input":"2023-05-06T16:19:57.530157Z","iopub.status.idle":"2023-05-06T16:19:57.534357Z","shell.execute_reply.started":"2023-05-06T16:19:57.530098Z","shell.execute_reply":"2023-05-06T16:19:57.533417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GLOVE_EMB = '/kaggle/working/glove.6B.300d.txt'\nEMBEDDING_DIM = 300\nLR = 1e-3\nBATCH_SIZE = 1024\nEPOCHS = 10\nMODEL_PATH = '.../output/kaggle/working/best_model.hdf5'","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:57.535771Z","iopub.execute_input":"2023-05-06T16:19:57.536383Z","iopub.status.idle":"2023-05-06T16:19:57.544239Z","shell.execute_reply.started":"2023-05-06T16:19:57.536323Z","shell.execute_reply":"2023-05-06T16:19:57.542716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\n\nf = open(GLOVE_EMB)\nfor line in f:\n  values = line.split()\n  word = value = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:19:57.545535Z","iopub.execute_input":"2023-05-06T16:19:57.545819Z","iopub.status.idle":"2023-05-06T16:20:24.680272Z","shell.execute_reply.started":"2023-05-06T16:19:57.54577Z","shell.execute_reply":"2023-05-06T16:20:24.679291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:20:24.681615Z","iopub.execute_input":"2023-05-06T16:20:24.682165Z","iopub.status.idle":"2023-05-06T16:20:25.162302Z","shell.execute_reply.started":"2023-05-06T16:20:24.682111Z","shell.execute_reply":"2023-05-06T16:20:25.161325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_layer = tf.keras.layers.Embedding(vocab_size,\n                                          EMBEDDING_DIM,\n                                          weights=[embedding_matrix],\n                                          input_length=MAX_SEQUENCE_LENGTH,\n                                          trainable=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:20:25.163737Z","iopub.execute_input":"2023-05-06T16:20:25.164353Z","iopub.status.idle":"2023-05-06T16:20:25.175099Z","shell.execute_reply.started":"2023-05-06T16:20:25.164162Z","shell.execute_reply":"2023-05-06T16:20:25.174236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout, Attention, Concatenate\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:21:15.091583Z","iopub.execute_input":"2023-05-06T16:21:15.092131Z","iopub.status.idle":"2023-05-06T16:21:15.097002Z","shell.execute_reply.started":"2023-05-06T16:21:15.0919Z","shell.execute_reply":"2023-05-06T16:21:15.096057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True,return_state=True))(x)\nx1 = x[0]\nprint(x1.shape)\nx1 = tf.reshape(x1,[-1,3328])\nprint(x1.shape)\nx = Concatenate(axis=-1)([x1,x[1],x[2],x[3],x[4]])\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel = tf.keras.Model(sequence_input, outputs)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:23:48.264426Z","iopub.execute_input":"2023-05-06T16:23:48.264787Z","iopub.status.idle":"2023-05-06T16:23:48.809582Z","shell.execute_reply.started":"2023-05-06T16:23:48.264727Z","shell.execute_reply":"2023-05-06T16:23:48.808573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:24:20.160227Z","iopub.execute_input":"2023-05-06T16:24:20.160963Z","iopub.status.idle":"2023-05-06T16:24:20.1832Z","shell.execute_reply.started":"2023-05-06T16:24:20.1607Z","shell.execute_reply":"2023-05-06T16:24:20.182352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:24:20.184959Z","iopub.execute_input":"2023-05-06T16:24:20.186101Z","iopub.status.idle":"2023-05-06T16:24:20.247331Z","shell.execute_reply.started":"2023-05-06T16:24:20.186051Z","shell.execute_reply":"2023-05-06T16:24:20.246024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:24:20.256917Z","iopub.execute_input":"2023-05-06T16:24:20.259044Z","iopub.status.idle":"2023-05-06T16:24:20.27266Z","shell.execute_reply.started":"2023-05-06T16:24:20.258991Z","shell.execute_reply":"2023-05-06T16:24:20.271565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:24:20.277353Z","iopub.execute_input":"2023-05-06T16:24:20.281193Z","iopub.status.idle":"2023-05-06T16:41:11.028014Z","shell.execute_reply.started":"2023-05-06T16:24:20.281134Z","shell.execute_reply":"2023-05-06T16:41:11.026791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights('./checkpoints/my_checkpoint')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-06T16:45:06.960005Z","iopub.status.idle":"2023-05-06T16:45:06.960566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(128, 5, activation='relu')(x)\nx = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2,return_sequences=True,return_state=True))(x)\nx1 = x[0]\nx1 = tf.reshape(x1,[-1,3328*2])\nx = Concatenate(axis=-1)([x1,x[1],x[2],x[3],x[4]])\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel1 = tf.keras.Model(sequence_input, outputs)\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel1.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T17:25:53.350014Z","iopub.execute_input":"2023-05-06T17:25:53.351003Z","iopub.status.idle":"2023-05-06T17:25:53.857949Z","shell.execute_reply.started":"2023-05-06T17:25:53.350385Z","shell.execute_reply":"2023-05-06T17:25:53.857038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model1.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","metadata":{"execution":{"iopub.status.busy":"2023-05-06T17:25:53.85976Z","iopub.execute_input":"2023-05-06T17:25:53.860129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.save_weights('./checkpoints/my_checkpoint1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True,return_state=True))(x)\nlstm1 = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(x[0])\nx = Concatenate(axis=-1)([lstm1,x[1],x[2],x[3],x[4]])\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel2 = tf.keras.Model(sequence_input, outputs)\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel2.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.save_weights('./checkpoints/my_checkpoint2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True,return_state=True))(x)\nlstm1 = LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True)(x[0])\nlstm2 = LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_state=True)(lstm1)\nconc = Concatenate(axis=-1)([lstm2[0],lstm2[1],lstm2[2],x[1],x[2],x[3],x[4]])\nx = Dense(512, activation='relu')(conc)\nx = Dropout(0.2)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel3 = tf.keras.Model(sequence_input, outputs)\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel3.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model3.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3.save_weights('./checkpoints/my_checkpoint3')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s, (at, al) = plt.subplots(2,1)\nat.plot(history.history['accuracy'], c= 'b')\nat.plot(history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n\nal.plot(history.history['loss'], c='m')\nal.plot(history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\nsentiment =[\"negative\", \"slightly negative\",\"neutral\",\"slightly positive\",\"positive\"]\n\ndef modified_sentiment(score):\n    return math.floor(score*5)\n\nscores2 = model.predict(x_test[0:100])\ny_pred_1d = [sentiment[modified_sentiment(score)] for score in scores]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, fontsize=13)\n    plt.yticks(tick_marks, classes, fontsize=13)\n\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=17)\n    plt.xlabel('Predicted label', fontsize=17)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(test_data.sentiment.to_list(), y_pred_1d)\nplt.figure(figsize=(6,6))\nplot_confusion_matrix(cnf_matrix, classes=test_data.sentiment.unique(), title=\"Confusion matrix\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(list(test_data.sentiment), y_pred_1d))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}